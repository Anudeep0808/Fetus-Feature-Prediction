{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347d555d",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830c777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8/8 [==============================] - 57s 7s/step - loss: -2189802.0000 - accuracy: 0.1101\n",
      "Validation loss: -2189802.0, Validation accuracy: 0.11013215780258179\n",
      "Epoch 2/5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Constants\n",
    "ANNOTATION_FILE = '/kaggle/input/fetus-dataset/Resized_ObjectDetection.xlsx'  \n",
    "IMAGE_DIR = '/kaggle/input/fetus-dataset/Resized_images/Resized_images'  \n",
    "\n",
    "# Load annotations for specific structures\n",
    "annotations = pd.read_excel(ANNOTATION_FILE)\n",
    "annotations = annotations[annotations['structure'].isin(['NT', 'nasal bone'])]\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = load_img(image_path, color_mode='rgb')  \n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "# Load and preprocess images\n",
    "images = []\n",
    "TARGET_STRUCTURES = ['NT', 'nasal bone']\n",
    "annotations_list = []\n",
    "\n",
    "# Unique filenames\n",
    "unique_filenames = annotations['fname'].unique()\n",
    "\n",
    "for fname in annotations['fname'].unique():\n",
    "    image_path = os.path.join(IMAGE_DIR, f\"{fname}.png\")\n",
    "    preprocessed_image = load_and_preprocess_image(image_path)\n",
    "    images.append(preprocessed_image) \n",
    "\n",
    "    image_annotations = annotations[annotations['fname'] == fname]\n",
    "    bbox = [0] * 8  \n",
    "\n",
    "    for structure in target_structures:\n",
    "        structure_annotations = image_annotations[image_annotations['structure'] == structure]\n",
    "        if not structure_annotations.empty:\n",
    "            \n",
    "            h_min, w_min, h_max, w_max = structure_annotations.iloc[0][['h_min', 'w_min', 'h_max', 'w_max']]\n",
    "            coords = [w_min, h_min, w_max, h_max]  \n",
    "            if structure == 'NT':\n",
    "                bbox[:4] = coords\n",
    "            else:\n",
    "                bbox[4:] = coords\n",
    "                \n",
    "    annotations_list.append(bbox)\n",
    "    \n",
    "# Convert to numpy arrays\n",
    "images = np.array(images)\n",
    "annotations_list = np.array(annotations_list)\n",
    "\n",
    "# Split into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(images, annotations_list, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Augmentation setup without flips\n",
    "train_datagen = ImageDataGenerator(\n",
    "    brightness_range=[0.8, 1.2],  \n",
    "    channel_shift_range=0.1,  \n",
    "    fill_mode='nearest'  \n",
    ")\n",
    "\n",
    "# Model Building - change the output layer to have 8 neurons\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "flat1 = Flatten()(base_model.output)\n",
    "class1 = Dense(1024, activation='relu')(flat1)\n",
    "output = Dense(8, activation='linear')(class1)  \n",
    "model = Model(inputs=base_model.inputs, outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['accuracy'])  # Use Mean Squared Error\n",
    "\n",
    "def reshape_annotations(annotations, num_features=2):  \n",
    "    \"\"\"Reshape annotations to match the model's output format.\"\"\"\n",
    "    reshaped_annotations = np.zeros((len(annotations), num_features * 4))\n",
    "    for i, box_pair in enumerate(annotations):\n",
    "        reshaped_annotations[i, :4] = box_pair[:4]  \n",
    "        reshaped_annotations[i, 4:] = box_pair[4:]  \n",
    "    return reshaped_annotations\n",
    "\n",
    "# Prepare the annotations for the training and validation sets\n",
    "y_train_reshaped = reshape_annotations(y_train, num_features=2)  \n",
    "y_val_reshaped = reshape_annotations(y_val, num_features=2)\n",
    "\n",
    "# Custom Training Loop\n",
    "BATCH_SIZE = 150\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    batches = 0\n",
    "\n",
    "    for x_batch in train_datagen.flow(x_train, batch_size=BATCH_SIZE, seed=epoch):\n",
    "        start = batches * BATCH_SIZE\n",
    "        end = (batches + 1) * BATCH_SIZE\n",
    "        y_batch = y_train_reshaped[start:end]\n",
    "\n",
    "        model.train_on_batch(x_batch, y_batch)\n",
    "\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / BATCH_SIZE:\n",
    "            break\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    val_loss, val_acc = model.evaluate(x_val, y_val_reshaped)\n",
    "    print(f\"Validation loss: {val_loss}, Validation accuracy: {val_acc}\")\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model.save('fetal_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dee4a",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a8b9be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EA012E5790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 559ms/step\n",
      "Annotated image saved at annotated_image.png\n",
      "Predicted bounding box coordinates saved at predicted_boxes.xlsx\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Constants\n",
    "MODEL_PATH = 'fetal_model.h5'  \n",
    "IMAGE_PATH = '100.png' \n",
    "OUTPUT_IMAGE_PATH = 'annotated_image.png'  \n",
    "OUTPUT_EXCEL_PATH = 'predicted_boxes.xlsx'  \n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Load and preprocess an image\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    image = load_img(image_path, target_size=target_size, color_mode='rgb')\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "# Predict bounding boxes for the new image\n",
    "preprocessed_image = preprocess_image(IMAGE_PATH)\n",
    "predicted_boxes = model.predict(preprocessed_image)[0]\n",
    "\n",
    "# Draw bounding boxes on the original image\n",
    "original_image = cv2.imread(IMAGE_PATH)\n",
    "for i in range(0, len(predicted_boxes), 4):\n",
    "    x_min, y_min, x_max, y_max = predicted_boxes[i:i+4].astype(int)\n",
    "    cv2.rectangle(original_image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "# Save the image with bounding boxes\n",
    "cv2.imwrite(OUTPUT_IMAGE_PATH, original_image)\n",
    "\n",
    "# Save the predicted bounding box coordinates to an Excel file\n",
    "# Assuming the first four values are for 'NT' and the next four are for 'nasal bone'\n",
    "df = pd.DataFrame([predicted_boxes], columns=['NT_x_min', 'NT_y_min', 'NT_x_max', 'NT_y_max', 'nasal_bone_x_min', 'nasal_bone_y_min', 'nasal_bone_x_max', 'nasal_bone_y_max'])\n",
    "df.to_excel(OUTPUT_EXCEL_PATH, index=False)\n",
    "\n",
    "print(f\"Annotated image saved at {OUTPUT_IMAGE_PATH}\")\n",
    "print(f\"Predicted bounding box coordinates saved at {OUTPUT_EXCEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b51634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
